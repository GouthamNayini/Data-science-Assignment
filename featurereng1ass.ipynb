{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "Missing values are values occurs in data set when some of the information is not stored  for a variable\n",
    "Its essential to handle missing the data values because the data which has missing values cant be used for modeling \n",
    "1.mean Imputation\n",
    "2.median Imputation\n",
    "3.mode Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "1.mean Imputation\n",
    "2.median Imputation\n",
    "3.mode Imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean Imputation\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset('titanic')\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age']=df['age'].fillna(df['age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#median  Imputation\n",
    "df['age']=df['age'].fillna(df['age'].median())\n",
    "df['age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch  fare embarked  class  \\\n",
       "61          1       1  female  38.0      0      0  80.0      NaN  First   \n",
       "829         1       1  female  62.0      0      0  80.0      NaN  First   \n",
       "\n",
       "       who  adult_male deck embark_town alive  alone  \n",
       "61   woman       False    B         NaN   yes   True  \n",
       "829  woman       False    B         NaN   yes   True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mode\n",
    "df[df['embarked'].isnull()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', nan], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_value=df['embarked'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      S\n",
       "1      C\n",
       "2      S\n",
       "3      S\n",
       "4      S\n",
       "      ..\n",
       "886    S\n",
       "887    S\n",
       "888    S\n",
       "889    C\n",
       "890    Q\n",
       "Name: embarked, Length: 891, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embarked'].fillna(mode_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "Imbalanced data refers to a situation where the distribution of classes within a dataset is skewed, meaning one class is significantly more prevalent than the others. This often occurs in real-world datasets where one class represents the majority of observations, while other classes are underrepresented. For example, in a medical dataset, the occurrence of a rare disease might be much lower compared to the occurrence of non-diseased instances.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to several issues:\n",
    "\n",
    "Biased Model: Machine learning models trained on imbalanced data tend to be biased towards the majority class. Since the majority class has more instances, the model may focus more on learning patterns related to that class, potentially ignoring minority classes altogether.\n",
    "\n",
    "Poor Generalization: Models trained on imbalanced data may not generalize well to new, unseen data. This is because they have not effectively learned the patterns associated with minority classes, leading to poor performance when encountering instances from those classes in the real world.\n",
    "\n",
    "Misleading Evaluation: Traditional evaluation metrics such as accuracy can be misleading when dealing with imbalanced data. A model that predicts the majority class for every instance may achieve high accuracy, but it fails to capture the predictive power for the minority classes, which are often the ones of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.?\n",
    "\n",
    "Up-sampling and down-sampling are two common techniques used to address class imbalance in datasets:\n",
    "\n",
    "Up-sampling (Over-sampling): Up-sampling involves increasing the number of instances in the minority class(es) to match the number of instances in the majority class. This is typically done by randomly duplicating instances from the minority class or by generating synthetic instances based on the existing minority class instances.\n",
    "\n",
    "Down-sampling (Under-sampling): Down-sampling involves reducing the number of instances in the majority class(es) to match the number of instances in the minority class. This is typically done by randomly removing instances from the majority class or by selecting a subset of instances from the majority class(es).\n",
    "\n",
    "Example scenarios for when up-sampling and down-sampling are required:\n",
    "\n",
    "Credit Card Fraud Detection:\n",
    "\n",
    "Up-sampling: In a dataset where instances of fraudulent transactions are rare compared to legitimate transactions, up-sampling can be used to increase the number of fraudulent transactions in the training data. This helps the model to learn the patterns associated with fraud more effectively.\n",
    "Down-sampling: Conversely, if the dataset contains a large number of fraudulent transactions and only a few legitimate transactions, down-sampling can be used to reduce the number of legitimate transactions. This helps prevent the model from being biased towards the majority class and improves its ability to detect fraudulent transactions accurately.\n",
    "Medical Diagnosis:\n",
    "\n",
    "Up-sampling: In a medical dataset where instances of a rare disease are significantly outnumbered by instances of non-diseased patients, up-sampling can be used to increase the number of instances of the rare disease. This ensures that the model learns to accurately identify the disease despite its low prevalence.\n",
    "Down-sampling: Conversely, if the dataset contains a large number of instances for a common condition and only a few instances of a rare condition, down-sampling can be used to balance the classes. This prevents the model from being biased towards the majority class and ensures that it effectively learns to identify both common and rare conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "Data augmentation is a technique commonly used in machine learning to artificially increase the size of a dataset by creating modified versions of existing data points. The goal of data augmentation is to improve the performance and generalization capabilities of machine learning models by exposing them to a wider variety of training examples.\n",
    "\n",
    "One popular method of data augmentation, especially in the context of addressing class imbalance, is Synthetic Minority Over-sampling Technique (SMOTE). SMOTE is specifically designed to address the class imbalance problem by generating synthetic samples for the minority class.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "Identify Minority Class Instances: First, SMOTE identifies instances belonging to the minority class in the dataset. These are the instances that are underrepresented compared to the majority class.\n",
    "\n",
    "Select Neighbor Instances: For each minority class instance, SMOTE selects its k nearest neighbors in the feature space. The number of neighbors, k, is typically chosen based on a user-defined parameter.\n",
    "\n",
    "Generate Synthetic Instances: For each minority class instance, SMOTE generates synthetic instances along the line segments connecting it to its selected neighbors in the feature space. These synthetic instances are created by combining features of the minority class instance with features of its nearest neighbors.\n",
    "\n",
    "Add Synthetic Instances to Dataset: After generating synthetic instances, SMOTE adds them to the original dataset, effectively increasing the number of instances in the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "\n",
    "Outliers are data points that significantly differ from other observations in a dataset. These observations lie at an abnormal distance from other data points, either in terms of magnitude or direction. Outliers can occur due to various reasons, including measurement errors, natural variability in the data, or rare events.\n",
    "\n",
    "It is essential to handle outliers for several reasons:\n",
    "\n",
    "Impact on Descriptive Statistics: Outliers can heavily influence summary statistics such as the mean and standard deviation, leading to misleading interpretations of the data. For instance, the mean can be skewed by extreme values, affecting the central tendency of the dataset.\n",
    "\n",
    "Impact on Machine Learning Models: Outliers can adversely affect the performance of machine learning models. Some algorithms, such as linear regression and k-means clustering, are sensitive to outliers and may produce biased or inaccurate results when outliers are present. Outliers can also affect the decision boundaries of classifiers, leading to suboptimal classification performance.\n",
    "\n",
    "Impact on Assumptions of Statistical Tests: Many statistical tests and modeling techniques assume that the data is normally distributed or follows a specific distribution. Outliers can violate these assumptions, leading to erroneous conclusions or inaccurate inferences.\n",
    "\n",
    "Reduced Robustness and Stability: Outliers can reduce the robustness and stability of statistical estimates and models. Models trained on datasets with outliers may be less reliable and more prone to overfitting, making them less useful for making predictions on new data.\n",
    "\n",
    "Data Quality and Interpretability: Outliers may indicate errors in data collection or measurement. Handling outliers ensures the integrity and quality of the dataset, making it more suitable for analysis and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "\n",
    "Handling missing data is crucial to ensure the accuracy and reliability of any analysis or model.  techniques commonly used to handle missing data in a dataset:\n",
    "\n",
    "Deletion:\n",
    "\n",
    "Listwise Deletion (Complete Case Analysis): In this approach, rows with any missing values are removed entirely from the dataset. While simple, this method may lead to loss of valuable information, especially if the missing data is not random.\n",
    "\n",
    "Pairwise Deletion: In this approach, only the missing values for specific variables are ignored when performing calculations or analysis. This method allows for maximum utilization of available data but may lead to biased estimates if missingness is related to certain variables.\n",
    "\n",
    "Imputation:\n",
    "\n",
    "Mean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the variable. This method is simple and preserves the overall distribution of the data but may lead to biased estimates, especially if the data is not normally distributed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n",
    "\n",
    "\n",
    "Determining whether missing data is missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR) is crucial for selecting appropriate strategies to handle missing data. Here are some strategies to help identify patterns in missing data:\n",
    "\n",
    "Visualizations:\n",
    "\n",
    "Missing Data Heatmap: Create a heatmap that visualizes the presence or absence of missing values across variables. This can help identify patterns or clusters of missing data.\n",
    "Pairwise Scatterplots: Plot pairwise scatterplots between variables with missing data to visualize relationships between missingness and other variables. This can reveal potential patterns or correlations in missing data.\n",
    "Descriptive Statistics:\n",
    "\n",
    "Missing Data Summary: Calculate summary statistics (e.g., mean, median, count) for variables with missing data and compare them to those for variables without missing data. Differences in distributions may indicate patterns in missingness.\n",
    "Correlation Analysis: Compute correlation coefficients between variables with missing data and other variables in the dataset. Significant correlations may suggest patterns in missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Strategies to evaluate the performance of a machine learning model on an imbalanced medical diagnosis dataset:\n",
    "\n",
    "Use appropriate evaluation metrics: Instead of relying solely on accuracy, which can be misleading in imbalanced datasets, use metrics such as precision, recall, F1-score, and area under the ROC curve (AUC-ROC). These metrics provide a more nuanced understanding of the model's performance, especially in scenarios with class imbalance.\n",
    "\n",
    "Confusion matrix analysis: Examine the confusion matrix to understand how the model is performing in terms of true positives, false positives, true negatives, and false negatives. This helps in identifying any biases or weaknesses in the model's predictions.\n",
    "\n",
    "ROC curve and precision-recall curve: Plot ROC curves and precision-recall curves to visualize the trade-off between true positive rate and false positive rate, as well as precision and recall, respectively. The AUC-ROC and area under the precision-recall curve (AUC-PR) provide quantitative measures of model performance that are robust to class imbalance.\n",
    "\n",
    "Stratified cross-validation: Use techniques such as stratified k-fold cross-validation to ensure that each fold of the cross-validation retains the same class distribution as the original dataset. This helps in obtaining reliable estimates of the model's performance across different subsets of data.\n",
    "\n",
    "Cost-sensitive learning: Incorporate the costs associated with misclassifications into the model training process. Adjusting class weights or using cost-sensitive learning algorithms can help prioritize the correct classification of minority class instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: Methods to balance an imbalanced dataset and down-sample the majority class to estimate customer satisfaction:\n",
    "\n",
    "Random under-sampling: Randomly remove instances from the majority class until the class distribution is balanced. This approach may lead to information loss but can be effective for large datasets.\n",
    "\n",
    "Cluster-based under-sampling: Use clustering algorithms to identify clusters within the majority class and then randomly sample instances from each cluster. This helps in preserving the diversity of the majority class while reducing its size.\n",
    "\n",
    "Tomek links: Identify pairs of instances (one from the majority class and one from the minority class) that are nearest neighbors but belong to different classes. Remove instances from the majority class in these pairs to improve the class separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?\n",
    "\n",
    "estimate the occurrence of a rare event:\n",
    "\n",
    "Random over-sampling: Randomly duplicate instances from the minority class until the class distribution is balanced. This approach may lead to overfitting, especially if the dataset is small.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): Generate synthetic instances for the minority class by interpolating between existing minority class instances. SMOTE helps in increasing the size of the minority class while preserving its underlying distribution.\n",
    "\n",
    "ADASYN (Adaptive Synthetic Sampling): Similar to SMOTE, ADASYN generates synthetic instances for the minority class but applies higher sampling weights to instances that are more challenging to classify. This helps in focusing the synthetic samples on regions of the feature space where the minority class is underrepresented."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
