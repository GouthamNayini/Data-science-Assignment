{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate.\n",
    "\n",
    "\n",
    "Linear Regression: Predicts continuous target variables (e.g., house price, temperature). It uses a straight line to model the relationship between the independent variables and the target variable.\n",
    "Logistic Regression: Predicts binary target variables (e.g., email spam/not spam, disease presence/absence). It uses a sigmoid function to model the probability of belonging to a specific class.\n",
    "Scenario: Predicting whether a patient has a specific disease based on symptoms. This requires a classification task (sick or healthy), making logistic regression more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?\n",
    "\n",
    "Cost Function: Logistic regression uses binary cross-entropy as the cost function. It measures the average difference between predicted probabilities and actual class labels.\n",
    "Optimization: Minimizing the cost function often involves iterative algorithms like gradient descent, adjusting model parameters in the direction that decreases the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
    "\n",
    "Regularization: Penalizes model complexity to prevent overfitting. In logistic regression, common techniques include L1 and L2 regularization.\n",
    "L1: Shrinks some parameters to zero, leading to feature selection.\n",
    "L2: Shrinks all parameters, reducing overall model complexity.\n",
    "Overfitting Prevention: Regularization helps the model better generalize to unseen data by avoiding memorizing specific training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "model?\n",
    "\n",
    "ROC Curve: Plots True Positive Rate (TPR) against False Positive Rate (FPR) at different prediction thresholds. It depicts the trade-off between correctly classifying positive and negative cases.\n",
    "Evaluation:\n",
    "Area Under the ROC Curve (AUC): A higher AUC (closer to 1) indicates better model performance.\n",
    "Precision-Recall Trade-off: Consider both precision (correctly identified positives) and recall (correctly identified true positives) depending on your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?\n",
    "\n",
    "L1 Regularization (LASSO): Penalizes the absolute value of coefficients, driving some to zero, effectively selecting essential features.\n",
    "L2 Regularization (Ridge): Shrinks all coefficients, reducing overall model complexity and potentially removing less important features.\n",
    "Wrapper Methods: Evaluate combinations of features using metrics like accuracy or information gain, selecting the best performing subset.\n",
    "Embedded Methods: Feature selection integrated within model training, such as using decision trees to identify important features.\n",
    "Filter Methods: Rank features based on individual metrics (e.g., correlation with the target variable) and select a top-K subset.\n",
    "These techniques help improve model performance by:\n",
    "\n",
    "Reducing Overfitting: Fewer features mean less complexity, reducing the risk of memorizing noise instead of learning genuine patterns.\n",
    "Interpretability: Understanding the impact of selected features becomes easier, offering insights into the model's decision-making process.\n",
    "Computational Efficiency: Training and prediction on fewer features is faster and can improve model scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?\n",
    "\n",
    "Imbalanced Data: When one class has significantly more samples than the other (e.g., few disease cases vs. many healthy individuals).\n",
    "Challenges: Model can bias predictions towards the majority class, neglecting the minority class.\n",
    "Strategies:\n",
    "\n",
    "Oversampling: Duplicate minority class samples to create a more balanced dataset, but can introduce overfitting.\n",
    "Undersampling: Randomly remove majority class samples, efficient but risks losing valuable information.\n",
    "SMOTE (Synthetic Minority Oversampling Technique): Create synthetic minority class samples based on existing data, often more effective than oversampling.\n",
    "Cost-Sensitive Learning: Assign higher weights to the minority class during training, penalizing misclassifications of minority samples more heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?\n",
    "\n",
    "Multicollinearity: When features are highly correlated, it can affect coefficient estimates and model stability.\n",
    "Data Issues: Missing values, outliers, and categorical variable encoding can impact model performance.\n",
    "Hyperparameter Tuning: Choosing the right regularization parameters and learning rate is crucial for optimal performance.\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Identify Collinear Features: Use correlation analysis or variance inflation factor (VIF) to identify correlated features.\n",
    "Remove or Combine Features: Remove highly correlated features, combine redundant ones, or use dimensionality reduction techniques like PCA.\n",
    "Regularization: L1 and L2 regularization can help mitigate the impact of multicollinearity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
